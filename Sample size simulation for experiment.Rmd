---
title: "Sample size simulation for experiment"
author: "BLINDED FOR PEER REVIEW"
output:
  html_document:
    toc: yes
    toc_depth: 3
    number_sections: yes
    toc_float: yes
    code_folding: hide
    theme: cosmo
---

Computing a sample size requirement for a mixed effects model cannot be done analytically (e.g., using G-Power or some other calculator). Instead, we need to turn to simulation in order to estimate a required sample size. Simulation is at the heart of understanding statistics and is a very powerful tool. In short, we will simulate the results of experiments under repeated sampling where we know the true effect size or "ground truth". This will allow us to compute estimates of sample size.  

***

<br>

# packages
First, we will need to install and/or load the packages that we need into RStudio.
```{r message=FALSE}
# Package names
packages <- c("ggplot2",  "tidyverse", "lme4", "lmerTest", "simr", "MBESS","future","future.apply","binom", "see")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
library(ggplot2)
library(lme4)
library(lmerTest)
library(tidyverse)
library(simr)
library(MBESS)
library(future)
library(future.apply)
library(binom)
library(see)

```

# import data and set coding
We will need to import the data from the in-class study. We download the [datafile ("data example.csv")](https://ele.exeter.ac.uk/course/view.php?id=10296) from ELE or Teams. We will need to run the dual process v2.0 model and then we will use this model to generate our simulations. 

```{r message=FALSE, results='hide'}
df <- read.csv("~/Library/CloudStorage/Dropbox/Documents/My data/Moral sense task/practical/data example.csv")
```

For ease of interpretation, we set deviation/simple coding for the factors.
```{r message=FALSE, results='hide'}
# make sure everything is factor
df$Task.Name <- as.factor(df$Task.Name)
df$Personal.force <- as.factor(df$Personal.force)
df$Intention <- as.factor(df$Intention)
df$DPv2.0.Permissible <- as.factor(df$DPv2.0.Permissible)

#set deviation contrasts for ease of interpretation -.5 vs .5
c<-contr.treatment(2)
my.coding<-matrix(rep(1/2, 2), ncol=1)
my.simple<-c-my.coding
my.simple

#keys  .5 Q-Yes,  -.5 P-Yes 
contrasts(df$Task.Name)<-my.simple
contrasts(df$Task.Name)

#personal force .5 Yes, -.5 No
contrasts(df$Personal.force)<-my.simple
contrasts(df$Personal.force)

#intention .5 Yes, -.5 No
contrasts(df$Intention)<-my.simple
contrasts(df$Intention)

#prediction DPv2.0 .5 Yes, -.5 No 
contrasts(df$DPv2.0.Permissible)<-my.simple
contrasts(df$DPv2.0.Permissible)

```
<br> 

# sample size simulations (subjects)
We will use the simr package to run simulations. This can be computationally intensive, especially for complex (e.g., maximal) models. Indeed, even when using the future package for parallel computation simulations can still sometimes take days to run!

## fit generalized linear mixed model (reduced model)
We fit the reduced (dropping the by-subject and by-dilemma random slope for the personal force x intention effect) generalized linear mixed model (GLMM) to form the basis for our sample size simulations.
```{r, attr.output='style="max-height: 500px;"'}
#fit reduced model without by-subject random slope for intent:domain 
fit<- glmer(Permissible ~ Personal.force*Intention +  (1+Personal.force+Intention|ID) + (1+Personal.force+Intention|Dilemma),  data=df, family = binomial(link = "logit"), control = glmerControl(optimizer = "nloptwrap", optCtrl = list(algorithm =  "NLOPT_LN_NELDERMEAD")))

summary(fit)
```
We test the effect of interest to make sure simr is getting the estimates correctly, this should be same as the model summary above.

```{r, attr.output='style="max-height: 500px;"'}
#test the effect of interest to make sure it's getting the model correctly, this should be same as standard model  
doTest(fit, fixed("Personal.force2", "z"))
```
Great, this is the same as in the model summary so simr reading the model object correctly.

# simulation parameters
We set up the parameters for the simulation. First we change the estimate for the personal force effect to our smallest effect size of interest (SESOI), OR = 3.5 or a "medium" sized effect. This is 1.25 in log odds.

```{r, attr.output='style="max-height: 500px;"'}
#change effect size to OR = 1.5 (sensitivity to detect a "small" effect)

teff <- "Personal.force2"
veff <- 1.25

fef <- fixef(fit)
fef[teff] <- veff
```
We take the random effects structure from the model's variance-covariance matrix.

```{r, attr.output='style="max-height: 500px;"'}
#grab random effects from variance-covariance matrix
vcv <- VarCorr(fit)
for (l in names(vcv)) {
  attr(vcv[[l]],"stddev") <- NULL
  attr(vcv[[l]],"correlation") <- NULL
}
```
We grab the dataframe from the model, retaining the fixed and random factors (there are no missing values in this data but if you have them you will need to use na.omit() here). We also extract the residual standard deviation.

```{r, attr.output='style="max-height: 500px;"'}
#dataframe from model
sdata <- cbind(fit@frame[,c("ID","Dilemma","Personal.force","Intention")])
```

We set-up some basic options for the simulation (number of sims, alpha, the levels of subjects or items to test, and a seed). We also set-up the optimizer and any other glmer options.

```{r, attr.output='style="max-height: 500px;"'}
# basic options
nsim <- 500
alpha <- 0.05
nitems <- c(25, 50, 75, 100)

# set seed 
set.seed(123)

#set glmer options
glmerctrlist <- glmerControl(optimizer = "nloptwrap", optCtrl = list(algorithm =  "NLOPT_LN_NELDERMEAD"))
```
We now have what we need to create the model structure for the simulation.

```{r, attr.output='style="max-height: 500px;"'}
#create the model structure
tglmer <- makeGlmer(attr(fit@frame,"formula"), 
                    family="binomial", fixef=fef, VarCorr=vcv, data=sdata)
```

Finally, we create a dataframe to save the output from the simulation. 

```{r, attr.output='style="max-height: 500px;"'}
# Create empty dataframe for power sim output
lnitem<-length(nitems)
poweroutput <- data.frame(nitem=nitems, mean=rep(NA_real_, lnitem),lower=rep(NA_real_, lnitem), upper=rep(NA_real_, lnitem), warnings=rep(NA_real_, lnitem), errors=rep(NA_real_, lnitem))
```

# run the simulation
Now we have everything we need to run the simulation - go read a book.   
```{r, attr.output='style="max-height: 500px;"'}
#power curve 25, 50, 75, and 100 participants (original dataset had 45 participants) with 104 scenarios

for (nitem in nitems) {
  
  print(nitem)
  
  tglx <- extend(tglmer,along="ID",n=nitem)
  
  plan(multisession)
  
  pstests <- future_replicate(nsim, powerSim(tglx, nsim=1,   
                              test=fixed("Personal.force2", "z"),
                              fitOpts=list(control=glmerctrlist),
                                             progress = FALSE),
                              future.globals = c("powerSim","tglx","teff","glmerctrlist"),
                              simplify = FALSE)
  plan(sequential)
  
  pvals <- sapply(pstests,function(x){x$pval})
  print(round(sum(pvals<alpha)/length(pvals),2))
  sucess <- sum(pvals<alpha)
  n <- length(pvals)
  interval <- binom.confint(sucess, n, level=0.95)[c("mean", "lower", "upper")]
  mean <- (round(mean(interval$mean),digits = 2))
  lower <- (round(mean(interval$lower),digits = 2))
  upper <- (round(mean(interval$upper),digits = 2))
  print(sprintf("power is %s, 95%% CIs [%s, %s]", mean, lower, upper))
  warnings <- sapply(pstests,function(x){length(unique(x$warnings$index))})
  errors <- sapply(pstests,function(x){length(unique(x$errors$index))})
  print(sprintf("with %s warnings and %s errors", sum(warnings), sum(errors)))
  output <- data.frame(nitem = nitem, mean = mean, lower=lower, upper = upper, warnings = sum(warnings), errors = sum(errors))
  poweroutput <- poweroutput %>% 
    rows_update(output)
}
```
We can plot the power curve.   

```{r, attr.output='style="max-height: 500px;"'}
#plot power curve
ggplot(poweroutput, aes(nitem, mean)) +        
  geom_point(size = 3, colour = "lightblue") +
  geom_line(linetype="dashed", colour = "lightblue") +
  lims(y = c(0, 1)) +
  labs(title = "", x = "number of participants", y = "power") +
  theme_bw() + 
  theme(panel.border = element_blank(), panel.grid.major = element_blank(),panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"))+
  geom_errorbar(aes(ymin = lower, ymax = upper),colour = "lightblue", width = 2)
```

The simulation reveals our power estimate and 95% CIs at various numbers of participants. The power curve is a nice graphical representation of how much benefit in terms of power there is in increasing sample size further.